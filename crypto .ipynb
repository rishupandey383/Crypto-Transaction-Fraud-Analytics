{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f82856a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded binance → shape: (2000, 8)\n",
      "                  time     open     high      low    close     volume  \\\n",
      "0  2024-10-18 15:00:00  0.03861  0.03872  0.03853  0.03861  2005.2370   \n",
      "1  2024-10-18 16:00:00  0.03860  0.03862  0.03844  0.03855  1241.2893   \n",
      "2  2024-10-18 17:00:00  0.03854  0.03855  0.03845  0.03852   798.4479   \n",
      "3  2024-10-18 18:00:00  0.03853  0.03880  0.03851  0.03867  2854.0002   \n",
      "4  2024-10-18 19:00:00  0.03867  0.03872  0.03860  0.03865   813.1840   \n",
      "\n",
      "      pair exchange  \n",
      "0  ETH-BTC  Binance  \n",
      "1  ETH-BTC  Binance  \n",
      "2  ETH-BTC  Binance  \n",
      "3  ETH-BTC  Binance  \n",
      "4  ETH-BTC  Binance  \n",
      "\n",
      "Loaded coinbase → shape: (2000, 8)\n",
      "                  time    open    high     low   close    volume     pair  \\\n",
      "0  2024-11-29 06:00:00  2.2789  2.2806  2.2789  2.2806      3.14  RLC-USD   \n",
      "1  2024-11-29 05:00:00  2.2857  2.2858  2.2721  2.2721   7485.58  RLC-USD   \n",
      "2  2024-11-29 04:00:00  2.2602  2.2854  2.2494  2.2854   2610.41  RLC-USD   \n",
      "3  2024-11-29 03:00:00  2.2813  2.2813  2.2538  2.2538  38423.76  RLC-USD   \n",
      "4  2024-11-29 02:00:00  2.2859  2.2917  2.2757  2.2814   8943.13  RLC-USD   \n",
      "\n",
      "   exchange  \n",
      "0  Coinbase  \n",
      "1  Coinbase  \n",
      "2  Coinbase  \n",
      "3  Coinbase  \n",
      "4  Coinbase  \n",
      "\n",
      "Loaded kraken → shape: (2000, 8)\n",
      "                  time   open   high    low  close  volume       pair exchange\n",
      "0  2024-10-30 07:00:00  0.243  0.243  0.243  0.243   0.243  1INCH-EUR   Kraken\n",
      "1  2024-10-30 08:00:00  0.242  0.242  0.242  0.242   0.242  1INCH-EUR   Kraken\n",
      "2  2024-10-30 09:00:00  0.242  0.242  0.242  0.242   0.000  1INCH-EUR   Kraken\n",
      "3  2024-10-30 10:00:00  0.242  0.242  0.242  0.242   0.000  1INCH-EUR   Kraken\n",
      "4  2024-10-30 11:00:00  0.242  0.242  0.242  0.242   0.000  1INCH-EUR   Kraken\n",
      "\n",
      "Loaded kucoin → shape: (1997, 8)\n",
      "                  time    open    high     low   close   volume      pair  \\\n",
      "0  2024-11-29 06:00:00  0.5888  0.5888  0.5842  0.5868   463.96  AVA-USDT   \n",
      "1  2024-11-29 05:00:00  0.5888  0.5888  0.5850  0.5850    86.40  AVA-USDT   \n",
      "2  2024-11-29 04:00:00  0.5871  0.5909  0.5848  0.5849  4043.04  AVA-USDT   \n",
      "3  2024-11-29 03:00:00  0.5934  0.5934  0.5883  0.5910  3118.29  AVA-USDT   \n",
      "4  2024-11-29 02:00:00  0.5890  0.5957  0.5890  0.5933  2655.89  AVA-USDT   \n",
      "\n",
      "  exchange  \n",
      "0   KuCoin  \n",
      "1   KuCoin  \n",
      "2   KuCoin  \n",
      "3   KuCoin  \n",
      "4   KuCoin  \n",
      "\n",
      "Combined dataset shape: (7997, 9)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'amount'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'amount'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     87\u001b[39m combined[\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(combined[\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m], errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Coerce amount to numeric\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m combined[\u001b[33m\"\u001b[39m\u001b[33mamount\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_numeric(\u001b[43mcombined\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamount\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Drop empty rows\u001b[39;00m\n\u001b[32m     93\u001b[39m combined = combined.dropna(subset=[\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mamount\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'amount'"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# CRYPTO TRANSACTION FRAUD ANALYSIS\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import networkx as nx\n",
    "\n",
    "# --------------------------------\n",
    "# 1. Load all exchange datasets\n",
    "# --------------------------------\n",
    "\n",
    "DATA_DIR = \"/mnt/data\"   # change if files are elsewhere\n",
    "files = {\n",
    "    \"binance\": r\"C:\\Users\\risha\\Downloads\\archive (9)\\data_2024\\binance.csv\",\n",
    "    \"coinbase\": r\"C:\\Users\\risha\\Downloads\\archive (9)\\data_2024\\coinbase.csv\",\n",
    "    \"kraken\": r\"C:\\Users\\risha\\Downloads\\archive (9)\\data_2024\\kraken.csv\",\n",
    "    \"kucoin\": r\"C:\\Users\\risha\\Downloads\\archive (9)\\data_2024\\kucoin.csv\"\n",
    "}\n",
    "\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for name, filename in files.items():\n",
    "    filepath = os.path.join(DATA_DIR, filename)\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    print(f\"\\nLoaded {name} → shape: {df.shape}\")\n",
    "    print(df.head())\n",
    "\n",
    "    dataframes[name] = df\n",
    "\n",
    "# --------------------------------\n",
    "# 2. Standardize column names\n",
    "# --------------------------------\n",
    "\n",
    "def normalize_columns(df):\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "    rename_map = {\n",
    "        \"timestamp\": \"timestamp\",\n",
    "        \"time\": \"timestamp\",\n",
    "        \"date\": \"timestamp\",\n",
    "        \"txid\": \"tx_id\",\n",
    "        \"tx_hash\": \"tx_id\",\n",
    "        \"transaction_id\": \"tx_id\",\n",
    "        \"from\": \"from_address\",\n",
    "        \"sender\": \"from_address\",\n",
    "        \"to\": \"to_address\",\n",
    "        \"receiver\": \"to_address\",\n",
    "        \"amount\": \"amount\",\n",
    "        \"qty\": \"amount\",\n",
    "        \"value\": \"amount\",\n",
    "        \"asset\": \"asset\",\n",
    "        \"currency\": \"asset\"\n",
    "    }\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in rename_map:\n",
    "            df.rename(columns={col: rename_map[col]}, inplace=True)\n",
    "\n",
    "    # Handle missing columns safely\n",
    "    if \"asset\" not in df.columns:\n",
    "        df[\"asset\"] = \"UNKNOWN\"\n",
    "\n",
    "    return df\n",
    "\n",
    "for name in dataframes:\n",
    "    dataframes[name] = normalize_columns(dataframes[name])\n",
    "\n",
    "# --------------------------------\n",
    "# 3. Merge into one combined dataset\n",
    "# --------------------------------\n",
    "\n",
    "combined = pd.concat(dataframes.values(), ignore_index=True)\n",
    "print(\"\\nCombined dataset shape:\", combined.shape)\n",
    "\n",
    "# --------------------------------\n",
    "# 4. Clean + Convert Data Types\n",
    "# --------------------------------\n",
    "\n",
    "# Convert timestamp\n",
    "combined[\"timestamp\"] = pd.to_datetime(combined[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Coerce amount to numeric\n",
    "combined[\"amount\"] = pd.to_numeric(combined[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "# Drop empty rows\n",
    "combined = combined.dropna(subset=[\"timestamp\", \"amount\"])\n",
    "\n",
    "# Add derived columns\n",
    "combined[\"date\"] = combined[\"timestamp\"].dt.date\n",
    "combined[\"hour\"] = combined[\"timestamp\"].dt.hour\n",
    "\n",
    "print(\"\\nCleaned dataset preview:\")\n",
    "print(combined.head())\n",
    "\n",
    "# --------------------------------\n",
    "# 5. Basic Insights\n",
    "# --------------------------------\n",
    "\n",
    "print(\"\\n===== BASIC INSIGHTS =====\")\n",
    "print(\"Total Transactions:\", len(combined))\n",
    "print(\"Total Volume:\", combined[\"amount\"].sum())\n",
    "print(\"Unique Assets:\", combined[\"asset\"].nunique())\n",
    "\n",
    "print(\"\\nTop 10 assets by volume:\")\n",
    "print(combined.groupby(\"asset\")[\"amount\"].sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# Daily transaction count\n",
    "daily_counts = combined.groupby(\"date\")[\"tx_id\"].count()\n",
    "print(\"\\nDaily transaction counts:\")\n",
    "print(daily_counts.head())\n",
    "\n",
    "# --------------------------------\n",
    "# 6. Visualization\n",
    "# --------------------------------\n",
    "\n",
    "# A) Transaction Amount Distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "combined[\"amount\"].plot(kind=\"hist\", bins=50)\n",
    "plt.title(\"Amount Distribution\")\n",
    "plt.xlabel(\"Amount\")\n",
    "plt.savefig(os.path.join(DATA_DIR, \"amount_distribution.png\"))\n",
    "plt.close()\n",
    "\n",
    "# B) Daily Transaction Count\n",
    "plt.figure(figsize=(10,5))\n",
    "daily_counts.plot()\n",
    "plt.title(\"Daily Transaction Count\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Transactions\")\n",
    "plt.savefig(os.path.join(DATA_DIR, \"daily_transactions.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --------------------------------\n",
    "# 7. Detect Suspicious Transactions (ANOMALY DETECTION)\n",
    "# --------------------------------\n",
    "\n",
    "print(\"\\n===== RUNNING ANOMALY DETECTION =====\")\n",
    "\n",
    "clean_for_ml = combined.dropna(subset=[\"amount\"]).copy()\n",
    "\n",
    "clean_for_ml[\"log_amount\"] = np.log1p(clean_for_ml[\"amount\"])\n",
    "\n",
    "model = IsolationForest(contamination=0.02, random_state=42)\n",
    "clean_for_ml[\"anomaly_score\"] = model.fit_predict(clean_for_ml[[\"amount\", \"log_amount\"]])\n",
    "\n",
    "# Anomalies = -1\n",
    "suspicious = clean_for_ml[clean_for_ml[\"anomaly_score\"] == -1]\n",
    "print(\"\\nSuspicious transactions detected:\", len(suspicious))\n",
    "\n",
    "# Save them\n",
    "suspicious.to_csv(os.path.join(DATA_DIR, \"suspicious_transactions.csv\"), index=False)\n",
    "print(\"Saved suspicious transactions → suspicious_transactions.csv\")\n",
    "\n",
    "# --------------------------------\n",
    "# 8. Graph Analysis (Address Network)\n",
    "# --------------------------------\n",
    "\n",
    "if \"from_address\" in combined.columns and \"to_address\" in combined.columns:\n",
    "    print(\"\\n===== GRAPH ANALYSIS =====\")\n",
    "\n",
    "    graph_df = combined.dropna(subset=[\"from_address\", \"to_address\"])\n",
    "    G = nx.from_pandas_edgelist(\n",
    "        graph_df,\n",
    "        source=\"from_address\",\n",
    "        target=\"to_address\",\n",
    "        edge_attr=\"amount\",\n",
    "        create_using=nx.DiGraph()\n",
    "    )\n",
    "\n",
    "    pagerank = nx.pagerank(G)\n",
    "    pr_df = pd.DataFrame.from_dict(pagerank, orient=\"index\", columns=[\"pagerank\"])\n",
    "    pr_df = pr_df.sort_values(by=\"pagerank\", ascending=False)\n",
    "\n",
    "    print(\"\\nTop 10 high-impact addresses (PageRank):\")\n",
    "    print(pr_df.head(10))\n",
    "\n",
    "    pr_df.to_csv(os.path.join(DATA_DIR, \"top_addresses_pagerank.csv\"))\n",
    "else:\n",
    "    print(\"\\nGraph analysis skipped (from/to columns missing).\")\n",
    "\n",
    "print(\"\\nAnalysis Complete. All files saved to:\", DATA_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
