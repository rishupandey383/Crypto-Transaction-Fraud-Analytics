{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f82856a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. CONFIG & IMPORTS — run once at the top of your notebook/script\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "sns.set(style=\"whitegrid\", rc={\"figure.figsize\":(10,5)})\n",
    "\n",
    "# Files dictionary — update paths to your CSV files (use raw strings on Windows)\n",
    "files = {\n",
    "    \"binance\": r\"C:\\Users\\risha\\Downloads\\archive (9)\\data_2024\\binance.csv\",\n",
    "    \"coinbase\": r\"C:\\Users\\risha\\Downloads\\archive (9)\\data_2024\\coinbase.csv\",\n",
    "    \"kraken\": r\"C:\\Users\\risha\\Downloads\\archive (9)\\data_2024\\kraken.csv\",\n",
    "    \"kucoin\": r\"C:\\Users\\risha\\Downloads\\archive (9)\\data_2024\\kucoin.csv\"\n",
    "}\n",
    "# Or autodiscover from a folder:\n",
    "# data_dir = \"/mnt/data\"\n",
    "# files = {os.path.splitext(os.path.basename(p))[0]: p for p in glob.glob(os.path.join(data_dir,\"*.csv\"))}\n",
    "\n",
    "OUTPUT_DIR = \"crypto_analysis_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Small helper\n",
    "def savefig_and_close(fig, path):\n",
    "    fig.savefig(path, bbox_inches=\"tight\", dpi=150)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aac13662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded binance: (2000, 8)\n",
      "Loaded coinbase: (2000, 8)\n",
      "Loaded kraken: (2000, 8)\n",
      "Loaded kucoin: (1997, 8)\n",
      "\n",
      "=== binance columns ===\n",
      " ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'pair', 'exchange', 'tx_id', 'asset']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>pair</th>\n",
       "      <th>exchange</th>\n",
       "      <th>tx_id</th>\n",
       "      <th>asset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-18 15:00:00</td>\n",
       "      <td>0.03861</td>\n",
       "      <td>0.03872</td>\n",
       "      <td>0.03853</td>\n",
       "      <td>0.03861</td>\n",
       "      <td>2005.2370</td>\n",
       "      <td>ETH-BTC</td>\n",
       "      <td>Binance</td>\n",
       "      <td>0</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-18 16:00:00</td>\n",
       "      <td>0.03860</td>\n",
       "      <td>0.03862</td>\n",
       "      <td>0.03844</td>\n",
       "      <td>0.03855</td>\n",
       "      <td>1241.2893</td>\n",
       "      <td>ETH-BTC</td>\n",
       "      <td>Binance</td>\n",
       "      <td>1</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp     open     high      low    close     volume  \\\n",
       "0  2024-10-18 15:00:00  0.03861  0.03872  0.03853  0.03861  2005.2370   \n",
       "1  2024-10-18 16:00:00  0.03860  0.03862  0.03844  0.03855  1241.2893   \n",
       "\n",
       "      pair exchange tx_id    asset  \n",
       "0  ETH-BTC  Binance     0  UNKNOWN  \n",
       "1  ETH-BTC  Binance     1  UNKNOWN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== coinbase columns ===\n",
      " ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'pair', 'exchange', 'tx_id', 'asset']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>pair</th>\n",
       "      <th>exchange</th>\n",
       "      <th>tx_id</th>\n",
       "      <th>asset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-29 06:00:00</td>\n",
       "      <td>2.2789</td>\n",
       "      <td>2.2806</td>\n",
       "      <td>2.2789</td>\n",
       "      <td>2.2806</td>\n",
       "      <td>3.14</td>\n",
       "      <td>RLC-USD</td>\n",
       "      <td>Coinbase</td>\n",
       "      <td>0</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-29 05:00:00</td>\n",
       "      <td>2.2857</td>\n",
       "      <td>2.2858</td>\n",
       "      <td>2.2721</td>\n",
       "      <td>2.2721</td>\n",
       "      <td>7485.58</td>\n",
       "      <td>RLC-USD</td>\n",
       "      <td>Coinbase</td>\n",
       "      <td>1</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp    open    high     low   close   volume     pair  \\\n",
       "0  2024-11-29 06:00:00  2.2789  2.2806  2.2789  2.2806     3.14  RLC-USD   \n",
       "1  2024-11-29 05:00:00  2.2857  2.2858  2.2721  2.2721  7485.58  RLC-USD   \n",
       "\n",
       "   exchange tx_id    asset  \n",
       "0  Coinbase     0  UNKNOWN  \n",
       "1  Coinbase     1  UNKNOWN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== kraken columns ===\n",
      " ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'pair', 'exchange', 'tx_id', 'asset']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>pair</th>\n",
       "      <th>exchange</th>\n",
       "      <th>tx_id</th>\n",
       "      <th>asset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-30 07:00:00</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.243</td>\n",
       "      <td>1INCH-EUR</td>\n",
       "      <td>Kraken</td>\n",
       "      <td>0</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-30 08:00:00</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.242</td>\n",
       "      <td>1INCH-EUR</td>\n",
       "      <td>Kraken</td>\n",
       "      <td>1</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp   open   high    low  close  volume       pair  \\\n",
       "0  2024-10-30 07:00:00  0.243  0.243  0.243  0.243   0.243  1INCH-EUR   \n",
       "1  2024-10-30 08:00:00  0.242  0.242  0.242  0.242   0.242  1INCH-EUR   \n",
       "\n",
       "  exchange tx_id    asset  \n",
       "0   Kraken     0  UNKNOWN  \n",
       "1   Kraken     1  UNKNOWN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== kucoin columns ===\n",
      " ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'pair', 'exchange', 'tx_id', 'asset']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>pair</th>\n",
       "      <th>exchange</th>\n",
       "      <th>tx_id</th>\n",
       "      <th>asset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-11-29 06:00:00</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>0.5842</td>\n",
       "      <td>0.5868</td>\n",
       "      <td>463.96</td>\n",
       "      <td>AVA-USDT</td>\n",
       "      <td>KuCoin</td>\n",
       "      <td>0</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-11-29 05:00:00</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>0.5850</td>\n",
       "      <td>0.5850</td>\n",
       "      <td>86.40</td>\n",
       "      <td>AVA-USDT</td>\n",
       "      <td>KuCoin</td>\n",
       "      <td>1</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp    open    high     low   close  volume      pair  \\\n",
       "0  2024-11-29 06:00:00  0.5888  0.5888  0.5842  0.5868  463.96  AVA-USDT   \n",
       "1  2024-11-29 05:00:00  0.5888  0.5888  0.5850  0.5850   86.40  AVA-USDT   \n",
       "\n",
       "  exchange tx_id    asset  \n",
       "0   KuCoin     0  UNKNOWN  \n",
       "1   KuCoin     1  UNKNOWN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. PREPROCESSING\n",
    "def find_first(cols, candidates):\n",
    "    l = {c.lower(): c for c in cols}\n",
    "    for cand in candidates:\n",
    "        if cand in l:\n",
    "            return l[cand]\n",
    "    return None\n",
    "\n",
    "COLUMN_CANDS = {\n",
    "    \"timestamp\": [\"timestamp\", \"time\", \"datetime\", \"date\", \"block_time\", \"block_timestamp\"],\n",
    "    \"tx_id\": [\"tx_hash\", \"hash\", \"transaction_hash\", \"txid\", \"tx_id\", \"id\"],\n",
    "    \"from_address\": [\"from\", \"from_address\", \"sender\", \"source\", \"src\", \"address_from\", \"sender_address\"],\n",
    "    \"to_address\": [\"to\", \"to_address\", \"recipient\", \"destination\", \"dst\", \"address_to\", \"recipient_address\"],\n",
    "    \"amount\": [\"value\", \"amount\", \"amt\", \"quantity\", \"token_amount\", \"value_usd\", \"amount_usd\"],\n",
    "    \"asset\": [\"token\", \"currency\", \"asset\", \"symbol\", \"token_symbol\"]\n",
    "}\n",
    "\n",
    "def standardize_df(df):\n",
    "    df = df.copy()\n",
    "    cols = list(df.columns)\n",
    "    mapping = {}\n",
    "    for canonical, cands in COLUMN_CANDS.items():\n",
    "        found = find_first(cols, cands)\n",
    "        if found:\n",
    "            mapping[found] = canonical\n",
    "    df = df.rename(columns=mapping)\n",
    "    # Ensure required columns exist\n",
    "    if \"tx_id\" not in df.columns:\n",
    "        df[\"tx_id\"] = df.index.astype(str)\n",
    "    if \"asset\" not in df.columns:\n",
    "        df[\"asset\"] = \"UNKNOWN\"\n",
    "    return df\n",
    "\n",
    "# Load files\n",
    "raw_dfs = {}\n",
    "for k,p in files.items():\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "        print(f\"Loaded {k}: {df.shape}\")\n",
    "        raw_dfs[k] = standardize_df(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed loading {k} from {p}: {e}\")\n",
    "\n",
    "# Quick preview\n",
    "for k,df in raw_dfs.items():\n",
    "    print(f\"\\n=== {k} columns ===\\n\", df.columns.tolist()[:40])\n",
    "    display(df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95b4332c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\AppData\\Local\\Temp\\ipykernel_20068\\2078592786.py:12: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(x, infer_datetime_format=True, errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binance: dropped 0 duplicate tx_ids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\AppData\\Local\\Temp\\ipykernel_20068\\2078592786.py:12: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(x, infer_datetime_format=True, errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coinbase: dropped 0 duplicate tx_ids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\AppData\\Local\\Temp\\ipykernel_20068\\2078592786.py:12: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(x, infer_datetime_format=True, errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kraken: dropped 0 duplicate tx_ids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\AppData\\Local\\Temp\\ipykernel_20068\\2078592786.py:12: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(x, infer_datetime_format=True, errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kucoin: dropped 0 duplicate tx_ids\n",
      "Saved sample for binance\n",
      "Saved sample for coinbase\n",
      "Saved sample for kraken\n",
      "Saved sample for kucoin\n"
     ]
    }
   ],
   "source": [
    "# 2. CLEANING\n",
    "def parse_ts_val(x):\n",
    "    if pd.isna(x):\n",
    "        return pd.NaT\n",
    "    # numeric unix ts handling\n",
    "    if isinstance(x,(int,float)) and x>1e9:\n",
    "        s = int(x)\n",
    "        if s>1e12:\n",
    "            return datetime.utcfromtimestamp(s/1000.0)\n",
    "        return datetime.utcfromtimestamp(s)\n",
    "    try:\n",
    "        return pd.to_datetime(x, infer_datetime_format=True, errors=\"coerce\")\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "cleaned_dfs = {}\n",
    "for name, df in raw_dfs.items():\n",
    "    c = df.copy()\n",
    "    # timestamp parse\n",
    "    if \"timestamp\" in c.columns:\n",
    "        c[\"timestamp_parsed\"] = c[\"timestamp\"].apply(parse_ts_val)\n",
    "    else:\n",
    "        c[\"timestamp_parsed\"] = pd.NaT\n",
    "    # amount numeric\n",
    "    if \"amount\" in c.columns:\n",
    "        c[\"amount\"] = pd.to_numeric(c[\"amount\"], errors=\"coerce\")\n",
    "    else:\n",
    "        c[\"amount\"] = np.nan\n",
    "    # ensure from/to exist (if not, create blank)\n",
    "    if \"from_address\" not in c.columns:\n",
    "        c[\"from_address\"] = np.nan\n",
    "    if \"to_address\" not in c.columns:\n",
    "        c[\"to_address\"] = np.nan\n",
    "    # drop exact duplicates\n",
    "    before = len(c)\n",
    "    c = c.drop_duplicates(subset=[\"tx_id\"])\n",
    "    after = len(c)\n",
    "    print(f\"{name}: dropped {before-after} duplicate tx_ids\")\n",
    "    # basic derived\n",
    "    c[\"date\"] = pd.to_datetime(c[\"timestamp_parsed\"]).dt.date\n",
    "    c[\"hour\"] = pd.to_datetime(c[\"timestamp_parsed\"]).dt.hour\n",
    "    c[\"log_amount\"] = np.log1p(c[\"amount\"].clip(lower=0))\n",
    "    cleaned_dfs[name] = c\n",
    "\n",
    "# Save cleaned samples\n",
    "for name,c in cleaned_dfs.items():\n",
    "    c.head(3).to_csv(os.path.join(OUTPUT_DIR, f\"{name}_cleaned_sample.csv\"), index=False)\n",
    "    print(\"Saved sample for\", name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfe07072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ANALYSIS: binance ---\n",
      "Rows: 2000\n",
      "Valid timestamps: 2000\n",
      "Valid amounts: 0\n",
      "Total volume: 0.0\n",
      "Unique assets: 1\n",
      "Not enough numeric rows for anomaly detection\n",
      "\n",
      "--- ANALYSIS: coinbase ---\n",
      "Rows: 2000\n",
      "Valid timestamps: 2000\n",
      "Valid amounts: 0\n",
      "Total volume: 0.0\n",
      "Unique assets: 1\n",
      "Not enough numeric rows for anomaly detection\n",
      "\n",
      "--- ANALYSIS: kraken ---\n",
      "Rows: 2000\n",
      "Valid timestamps: 2000\n",
      "Valid amounts: 0\n",
      "Total volume: 0.0\n",
      "Unique assets: 1\n",
      "Not enough numeric rows for anomaly detection\n",
      "\n",
      "--- ANALYSIS: kucoin ---\n",
      "Rows: 1997\n",
      "Valid timestamps: 1997\n",
      "Valid amounts: 0\n",
      "Total volume: 0.0\n",
      "Unique assets: 1\n",
      "Not enough numeric rows for anomaly detection\n"
     ]
    }
   ],
   "source": [
    "# 3. ANALYSIS\n",
    "# Per-exchange EDA & plotting\n",
    "for name, df in cleaned_dfs.items():\n",
    "    print(f\"\\n--- ANALYSIS: {name} ---\")\n",
    "    print(\"Rows:\", len(df))\n",
    "    print(\"Valid timestamps:\", df['timestamp_parsed'].notna().sum())\n",
    "    print(\"Valid amounts:\", df['amount'].notna().sum())\n",
    "    print(\"Total volume:\", df['amount'].sum(skipna=True))\n",
    "    print(\"Unique assets:\", df['asset'].nunique())\n",
    "\n",
    "    # Amount distribution plot\n",
    "    fig = plt.figure()\n",
    "    sns.histplot(df[\"amount\"].dropna(), bins=100, log_scale=(False,True))\n",
    "    plt.title(f\"{name} — Transaction Amount Distribution\")\n",
    "    savefig_and_close(fig, os.path.join(OUTPUT_DIR, f\"{name}_amount_hist.png\"))\n",
    "\n",
    "    # Daily tx counts & volume\n",
    "    if df['timestamp_parsed'].notna().any():\n",
    "        daily = df.groupby(\"date\").agg(daily_tx=(\"tx_id\",\"count\"), daily_vol=(\"amount\",\"sum\")).reset_index()\n",
    "        fig = plt.figure()\n",
    "        plt.plot(pd.to_datetime(daily['date']), daily['daily_tx'])\n",
    "        plt.title(f\"{name} — Daily Transaction Count\")\n",
    "        savefig_and_close(fig, os.path.join(OUTPUT_DIR, f\"{name}_daily_tx.png\"))\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.plot(pd.to_datetime(daily['date']), daily['daily_vol'])\n",
    "        plt.title(f\"{name} — Daily Transaction Volume\")\n",
    "        savefig_and_close(fig, os.path.join(OUTPUT_DIR, f\"{name}_daily_volume.png\"))\n",
    "        # store daily for summary\n",
    "        daily.to_csv(os.path.join(OUTPUT_DIR, f\"{name}_daily_summary.csv\"), index=False)\n",
    "\n",
    "    # Top senders & receivers by volume\n",
    "    if df[\"from_address\"].notna().any():\n",
    "        top_senders = df.groupby(\"from_address\").agg(tx_count=(\"tx_id\",\"count\"), total_out=(\"amount\",\"sum\")).reset_index().sort_values(\"total_out\", ascending=False).head(20)\n",
    "        top_senders.to_csv(os.path.join(OUTPUT_DIR, f\"{name}_top_senders.csv\"), index=False)\n",
    "    if df[\"to_address\"].notna().any():\n",
    "        top_receivers = df.groupby(\"to_address\").agg(tx_count=(\"tx_id\",\"count\"), total_in=(\"amount\",\"sum\")).reset_index().sort_values(\"total_in\", ascending=False).head(20)\n",
    "        top_receivers.to_csv(os.path.join(OUTPUT_DIR, f\"{name}_top_receivers.csv\"), index=False)\n",
    "\n",
    "    # Simple anomaly detection: z-score + IsolationForest\n",
    "    df_for_anom = df.dropna(subset=[\"amount\"]).copy()\n",
    "    if len(df_for_anom) >= 50:\n",
    "        df_for_anom[\"amount_z\"] = stats.zscore(df_for_anom[\"amount\"].fillna(0))\n",
    "        df_for_anom[\"z_outlier\"] = df_for_anom[\"amount_z\"].abs() > 4.0\n",
    "\n",
    "        # Isolation Forest on amount & log_amount\n",
    "        X = df_for_anom[[\"amount\",\"log_amount\"]].fillna(0).values\n",
    "        scaler = StandardScaler()\n",
    "        Xs = scaler.fit_transform(X)\n",
    "        iso = IsolationForest(n_estimators=200, contamination=0.01, random_state=42)\n",
    "        preds = iso.fit_predict(Xs)\n",
    "        df_for_anom[\"iso_outlier\"] = preds == -1\n",
    "\n",
    "        anomalies = df_for_anom[(df_for_anom[\"z_outlier\"]) | (df_for_anom[\"iso_outlier\"])]\n",
    "        anomalies.to_csv(os.path.join(OUTPUT_DIR, f\"{name}_anomalies.csv\"), index=False)\n",
    "        print(f\"Anomalies flagged: {len(anomalies)}. Saved to {name}_anomalies.csv\")\n",
    "    else:\n",
    "        print(\"Not enough numeric rows for anomaly detection\")\n",
    "\n",
    "    # Graph analysis (if addresses exist)\n",
    "    if df[\"from_address\"].notna().any() and df[\"to_address\"].notna().any():\n",
    "        edges = df.dropna(subset=[\"from_address\",\"to_address\"])[[\"from_address\",\"to_address\",\"amount\"]].copy()\n",
    "        G = nx.DiGraph()\n",
    "        for _, r in edges.iterrows():\n",
    "            u,v,a = r[\"from_address\"], r[\"to_address\"], float(r[\"amount\"]) if not pd.isna(r[\"amount\"]) else 0.0\n",
    "            if G.has_edge(u,v):\n",
    "                G[u][v][\"weight\"] += a\n",
    "                G[u][v][\"count\"] += 1\n",
    "            else:\n",
    "                G.add_edge(u,v, weight=a, count=1)\n",
    "        print(\"Graph nodes:\", len(G.nodes()), \"edges:\", len(G.edges()))\n",
    "        # PageRank\n",
    "        try:\n",
    "            pr = nx.pagerank(G, max_iter=200)\n",
    "            pr_df = pd.DataFrame.from_dict(pr, orient=\"index\", columns=[\"pagerank\"]).reset_index().rename(columns={\"index\":\"address\"})\n",
    "            pr_df = pr_df.sort_values(\"pagerank\", ascending=False).head(50)\n",
    "            pr_df.to_csv(os.path.join(OUTPUT_DIR, f\"{name}_pagerank_top50.csv\"), index=False)\n",
    "            print(\"Saved pagerank top50\")\n",
    "        except Exception as e:\n",
    "            print(\"Pagerank failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f04dc80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined_cleaned.csv — rows: 7997\n",
      "Saved address_level_features.csv — rows: 0\n",
      "Saved suspicious_addresses_ranked.csv — top rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>out_tx_count</th>\n",
       "      <th>out_total</th>\n",
       "      <th>out_mean</th>\n",
       "      <th>in_tx_count</th>\n",
       "      <th>in_total</th>\n",
       "      <th>in_mean</th>\n",
       "      <th>net_flow</th>\n",
       "      <th>tx_count_total</th>\n",
       "      <th>median_interarrival_s</th>\n",
       "      <th>out_tx_count_for_ia</th>\n",
       "      <th>_net_flow_mag</th>\n",
       "      <th>_rank_net_flow</th>\n",
       "      <th>_rank_tx_count</th>\n",
       "      <th>_inv_interarrival</th>\n",
       "      <th>_rank_burst</th>\n",
       "      <th>suspicious_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [address, out_tx_count, out_total, out_mean, in_tx_count, in_total, in_mean, net_flow, tx_count_total, median_interarrival_s, out_tx_count_for_ia, _net_flow_mag, _rank_net_flow, _rank_tx_count, _inv_interarrival, _rank_burst, suspicious_score]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AGGREGATE INSIGHTS ===\n",
      "\n",
      "Top assets by total volume:\n",
      "asset\n",
      "UNKNOWN    0.0\n",
      "Name: amount, dtype: float64\n",
      "\n",
      "Most active addresses (by total tx count):\n",
      "Empty DataFrame\n",
      "Columns: [address, tx_count_total]\n",
      "Index: []\n",
      "\n",
      "=== RECOMMENDED NEXT ACTIONS ===\n",
      "1) Manually review the top 100 suspicious addresses from suspicious_addresses_top200.csv\n",
      "2) For each flagged address, extract transaction trails (in/out flows) and visualize subgraph to detect layering\n",
      "3) Cross-check flagged addresses with public tag/blacklist databases (Chainalysis, Etherscan labels, etc.)\n",
      "4) Tune anomaly detection: create more features (unique counterparties, % of value to exchanges, time-of-day patterns)\n",
      "5) If labelled fraud data becomes available, train a supervised classifier (XGBoost/LightGBM) and evaluate\n",
      "6) Build an interactive visualization of the top suspicious subnetworks for investigators\n",
      "\n",
      "Saved address_level_features_final.csv (cleaned)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4. INSIGHT EXTRACTION (fixed)\n",
    "# -----------------------------\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Expecting:\n",
    "# - cleaned_dfs: dict of {exchange_name: DataFrame} from previous cleaning step\n",
    "# - OUTPUT_DIR: directory path where outputs should be saved\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Combine cleaned data safely\n",
    "combined = pd.concat(cleaned_dfs.values(), ignore_index=True, sort=False)\n",
    "combined.to_csv(os.path.join(OUTPUT_DIR, \"combined_cleaned.csv\"), index=False)\n",
    "print(\"Saved combined_cleaned.csv — rows:\", len(combined))\n",
    "\n",
    "# 2) Prepare address aggregation input\n",
    "# Ensure address columns exist\n",
    "if \"from_address\" not in combined.columns:\n",
    "    combined[\"from_address\"] = pd.NA\n",
    "if \"to_address\" not in combined.columns:\n",
    "    combined[\"to_address\"] = pd.NA\n",
    "\n",
    "# Normalize address columns to pandas string dtype to avoid dtype mismatches\n",
    "combined[\"from_address\"] = combined[\"from_address\"].astype(\"string\")\n",
    "combined[\"to_address\"]   = combined[\"to_address\"].astype(\"string\")\n",
    "\n",
    "# Replace genuinely missing-like values with <NA> so joins behave consistently\n",
    "combined[\"from_address\"] = combined[\"from_address\"].replace({\"\": pd.NA, \"nan\": pd.NA})\n",
    "combined[\"to_address\"]   = combined[\"to_address\"].replace({\"\": pd.NA, \"nan\": pd.NA})\n",
    "\n",
    "# 3) Compute outbound aggregates (per-sender)\n",
    "if \"tx_id\" not in combined.columns:\n",
    "    combined[\"tx_id\"] = combined.index.astype(str)\n",
    "\n",
    "out_agg = pd.DataFrame()\n",
    "in_agg  = pd.DataFrame()\n",
    "\n",
    "if combined[\"from_address\"].notna().any():\n",
    "    out_agg = (combined\n",
    "               .dropna(subset=[\"from_address\"])\n",
    "               .groupby(\"from_address\", dropna=True)\n",
    "               .agg(out_tx_count = (\"tx_id\", \"nunique\"),\n",
    "                    out_total    = (\"amount\", \"sum\"),\n",
    "                    out_mean     = (\"amount\", \"mean\"))\n",
    "               .reset_index()\n",
    "               .rename(columns={\"from_address\": \"address\"}))\n",
    "else:\n",
    "    out_agg = pd.DataFrame(columns=[\"address\",\"out_tx_count\",\"out_total\",\"out_mean\"])\n",
    "\n",
    "if combined[\"to_address\"].notna().any():\n",
    "    in_agg = (combined\n",
    "              .dropna(subset=[\"to_address\"])\n",
    "              .groupby(\"to_address\", dropna=True)\n",
    "              .agg(in_tx_count = (\"tx_id\", \"nunique\"),\n",
    "                   in_total    = (\"amount\", \"sum\"),\n",
    "                   in_mean     = (\"amount\", \"mean\"))\n",
    "              .reset_index()\n",
    "              .rename(columns={\"to_address\": \"address\"}))\n",
    "else:\n",
    "    in_agg = pd.DataFrame(columns=[\"address\",\"in_tx_count\",\"in_total\",\"in_mean\"])\n",
    "\n",
    "# Coerce 'address' columns to string in both before merging\n",
    "out_agg[\"address\"] = out_agg[\"address\"].astype(\"string\")\n",
    "in_agg[\"address\"]  = in_agg[\"address\"].astype(\"string\")\n",
    "\n",
    "# 4) Merge inbound/outbound features\n",
    "addr_feat = pd.merge(out_agg, in_agg, on=\"address\", how=\"outer\")\n",
    "# fill numeric NaNs with 0\n",
    "num_cols = [c for c in addr_feat.columns if c not in (\"address\",)]\n",
    "addr_feat[num_cols] = addr_feat[num_cols].fillna(0)\n",
    "\n",
    "# 5) Net flow and total tx count\n",
    "addr_feat[\"net_flow\"] = addr_feat[\"in_total\"] - addr_feat[\"out_total\"]\n",
    "addr_feat[\"tx_count_total\"] = addr_feat[\"in_tx_count\"] + addr_feat[\"out_tx_count\"]\n",
    "\n",
    "# 6) Median interarrival (sending behavior) for addresses with timestamp\n",
    "ia = []\n",
    "if \"timestamp_parsed\" in combined.columns and combined[\"timestamp_parsed\"].notna().any():\n",
    "    # Work with rows that have both from_address and timestamp\n",
    "    tmp = combined.dropna(subset=[\"from_address\",\"timestamp_parsed\"]).sort_values(\"timestamp_parsed\")\n",
    "    # convert timestamp_parsed to integer seconds for diff\n",
    "    tmp_times = tmp.assign(ts_seconds = pd.to_datetime(tmp[\"timestamp_parsed\"]).astype(\"int64\") // 1_000_000_000)\n",
    "    for addr, grp in tmp_times.groupby(\"from_address\", dropna=True):\n",
    "        times = grp[\"ts_seconds\"].to_numpy()\n",
    "        if len(times) > 1:\n",
    "            diffs = np.diff(times)\n",
    "            median_diff = float(np.median(diffs))\n",
    "            ia.append((str(addr), median_diff, int(len(times))))\n",
    "# Build ia_df\n",
    "ia_df = pd.DataFrame(ia, columns=[\"address\",\"median_interarrival_s\",\"out_tx_count_for_ia\"])\n",
    "if not ia_df.empty:\n",
    "    ia_df[\"address\"] = ia_df[\"address\"].astype(\"string\")\n",
    "\n",
    "# 7) Merge interarrival into addr_feat safely (coerce address dtype first)\n",
    "addr_feat[\"address\"] = addr_feat[\"address\"].astype(\"string\")\n",
    "if not ia_df.empty:\n",
    "    addr_feat = addr_feat.merge(ia_df, on=\"address\", how=\"left\")\n",
    "else:\n",
    "    # ensure columns exist\n",
    "    addr_feat[\"median_interarrival_s\"] = pd.NA\n",
    "    addr_feat[\"out_tx_count_for_ia\"] = 0\n",
    "\n",
    "# Fill missing median_interarrival_s with a large number (so low burstiness)\n",
    "addr_feat[\"median_interarrival_s\"] = addr_feat[\"median_interarrival_s\"].fillna(1e9).astype(float)\n",
    "addr_feat[\"out_tx_count_for_ia\"] = addr_feat[\"out_tx_count_for_ia\"].fillna(0).astype(int)\n",
    "\n",
    "# 8) Save address-level features\n",
    "addr_feat.to_csv(os.path.join(OUTPUT_DIR, \"address_level_features.csv\"), index=False)\n",
    "print(\"Saved address_level_features.csv — rows:\", len(addr_feat))\n",
    "\n",
    "# 9) Create a suspiciousness heuristic score (normalized ranks)\n",
    "# Components: net_flow (higher inflow-outflow), tx_count_total (higher activity), burstiness (low median_interarrival)\n",
    "# Normalize each component to percentile ranks (0..1)\n",
    "def pct_rank(series):\n",
    "    if series.nunique() <= 1:\n",
    "        return pd.Series(0.0, index=series.index)\n",
    "    return series.rank(pct=True)\n",
    "\n",
    "# Use absolute net_flow magnitude because large in or outflows can be suspicious; here we prefer net outflow magnitude\n",
    "addr_feat[\"_net_flow_mag\"] = addr_feat[\"net_flow\"].abs()\n",
    "addr_feat[\"_rank_net_flow\"] = pct_rank(addr_feat[\"_net_flow_mag\"])\n",
    "addr_feat[\"_rank_tx_count\"] = pct_rank(addr_feat[\"tx_count_total\"])\n",
    "\n",
    "# For burstiness: smaller median_interarrival -> more bursty -> higher suspiciousness\n",
    "# Convert median_interarrival_s to inverse and rank\n",
    "addr_feat[\"_inv_interarrival\"] = 1.0 / (addr_feat[\"median_interarrival_s\"] + 1.0)   # +1 to avoid div by zero\n",
    "addr_feat[\"_rank_burst\"] = pct_rank(addr_feat[\"_inv_interarrival\"])\n",
    "\n",
    "# Weighted score (weights can be tuned)\n",
    "w_net = 0.45\n",
    "w_tx  = 0.35\n",
    "w_bst = 0.20\n",
    "\n",
    "addr_feat[\"suspicious_score\"] = (\n",
    "    addr_feat[\"_rank_net_flow\"] * w_net +\n",
    "    addr_feat[\"_rank_tx_count\"] * w_tx +\n",
    "    addr_feat[\"_rank_burst\"] * w_bst\n",
    ")\n",
    "\n",
    "# 10) Rank and save top suspicious addresses\n",
    "suspicious_addresses = addr_feat.sort_values(\"suspicious_score\", ascending=False).reset_index(drop=True)\n",
    "suspicious_addresses.to_csv(os.path.join(OUTPUT_DIR, \"suspicious_addresses_ranked.csv\"), index=False)\n",
    "print(\"Saved suspicious_addresses_ranked.csv — top rows:\")\n",
    "display(suspicious_addresses.head(10))\n",
    "\n",
    "# 11) Save top-n lists for manual review\n",
    "suspicious_addresses.head(200).to_csv(os.path.join(OUTPUT_DIR, \"suspicious_addresses_top200.csv\"), index=False)\n",
    "addr_feat.sort_values(\"tx_count_total\", ascending=False).head(200).to_csv(os.path.join(OUTPUT_DIR, \"most_active_addresses_top200.csv\"), index=False)\n",
    "\n",
    "# 12) High-level aggregate insights\n",
    "print(\"\\n=== AGGREGATE INSIGHTS ===\")\n",
    "# Top assets by volume\n",
    "if \"asset\" in combined.columns:\n",
    "    top_assets = combined.groupby(\"asset\")[\"amount\"].sum().sort_values(ascending=False).head(20)\n",
    "    print(\"\\nTop assets by total volume:\")\n",
    "    print(top_assets)\n",
    "\n",
    "# Top senders / receivers globally\n",
    "if combined[\"from_address\"].notna().any():\n",
    "    top_senders_global = combined.groupby(\"from_address\")[\"amount\"].sum().sort_values(ascending=False).head(10)\n",
    "    print(\"\\nTop 10 senders by volume:\")\n",
    "    print(top_senders_global)\n",
    "\n",
    "if combined[\"to_address\"].notna().any():\n",
    "    top_receivers_global = combined.groupby(\"to_address\")[\"amount\"].sum().sort_values(ascending=False).head(10)\n",
    "    print(\"\\nTop 10 receivers by volume:\")\n",
    "    print(top_receivers_global)\n",
    "\n",
    "# Most active addresses\n",
    "print(\"\\nMost active addresses (by total tx count):\")\n",
    "most_active = addr_feat.sort_values(\"tx_count_total\", ascending=False)[[\"address\",\"tx_count_total\"]].head(10)\n",
    "print(most_active.to_string(index=False))\n",
    "\n",
    "# 13) Recommended next actions (printed)\n",
    "print(\"\\n=== RECOMMENDED NEXT ACTIONS ===\")\n",
    "print(\"1) Manually review the top 100 suspicious addresses from suspicious_addresses_top200.csv\")\n",
    "print(\"2) For each flagged address, extract transaction trails (in/out flows) and visualize subgraph to detect layering\")\n",
    "print(\"3) Cross-check flagged addresses with public tag/blacklist databases (Chainalysis, Etherscan labels, etc.)\")\n",
    "print(\"4) Tune anomaly detection: create more features (unique counterparties, % of value to exchanges, time-of-day patterns)\")\n",
    "print(\"5) If labelled fraud data becomes available, train a supervised classifier (XGBoost/LightGBM) and evaluate\")\n",
    "print(\"6) Build an interactive visualization of the top suspicious subnetworks for investigators\")\n",
    "\n",
    "# 14) Clean up temporary helper columns before final save\n",
    "for col in [\"_net_flow_mag\",\"_rank_net_flow\",\"_rank_tx_count\",\"_inv_interarrival\",\"_rank_burst\"]:\n",
    "    if col in addr_feat.columns:\n",
    "        addr_feat.drop(columns=[col], inplace=True)\n",
    "\n",
    "addr_feat.to_csv(os.path.join(OUTPUT_DIR, \"address_level_features_final.csv\"), index=False)\n",
    "print(\"\\nSaved address_level_features_final.csv (cleaned)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
